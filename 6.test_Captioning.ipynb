{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_Captioning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBeaQj3dtwdxyrMxcQQYAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spnetic-5/CaptionBot.ai/blob/main/6.test_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNALMayAfvnv"
      },
      "source": [
        "#Generate Captions for a Fresh Image\n",
        "\n",
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmRMg00ZgDpx"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_features(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YUuH-nSgFWB"
      },
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from training)\n",
        "max_length = 34\n",
        "# load the model\n",
        "model = load_model('model_18.h5')\n",
        "# load and prepare the photograph\n",
        "photo = extract_features('Sample_Image.jpg')\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yisSF8TlgG_w"
      },
      "source": [
        "#Remove startseq and endseq\n",
        "query = description\n",
        "stopwords = ['start','end']\n",
        "querywords = query.split()\n",
        "\n",
        "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
        "result = ' '.join(resultwords)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}